{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP - INTRODUCTION TO SUPERVISED LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **Showing that $ \\hat{\\beta} $ is unbiased**\n",
    "\n",
    "We know that $ \\hat{\\beta}^* $ is unbiased, i.e., $ E[\\hat{\\beta}^*] = \\beta $. For $ \\hat{\\beta} $, it's defined that $ \\hat{\\beta} = Cy $, where $ C = H + D $ and $ H = (X^TX)^{-1}X^T $. We need to prove $ E[\\hat{\\beta}] = \\beta $ to confirm that $ \\hat{\\beta} $ is unbiased.\n",
    "\n",
    "Since $ y = X\\beta + \\epsilon $ and $ E[\\epsilon] = 0 $, we have:\n",
    "$$ E[\\hat{\\beta}] = E[Cy] = E[C(X\\beta + \\epsilon)] = CE[X\\beta] + CE[\\epsilon] $$\n",
    "$$ E[\\hat{\\beta}] = CX\\beta + C\\cdot0 $$  \n",
    "$$ E[\\hat{\\beta}] = (H + D)X\\beta $$\n",
    "$$ E[\\hat{\\beta}] = HX\\beta + DX\\beta $$\n",
    "$$ E[\\hat{\\beta}] = \\beta + DX\\beta $$\n",
    "\n",
    "For $ \\hat{\\beta} $ to be unbiased, $ DX\\beta $ must equal zero. Since we are not given any specifics about $ D $, we would assume that for $ \\hat{\\beta} $ to be unbiased, $ D $ must be such that $ DX = 0 $. Hence, $ E[\\hat{\\beta}] = \\beta $, and $ \\hat{\\beta} $ is unbiased.\n",
    "\n",
    "2. **Computing $ Var(\\hat{\\beta}^*) $ and $ Var(\\hat{\\beta}) $**\n",
    "\n",
    "For $ \\hat{\\beta}^* $, we use the fact that the variance of a linear transformation $ Ax $ of a random vector $ x $ with variance-covariance matrix $ \\Sigma $ is given by $ A\\Sigma A^T $. Assuming $ \\epsilon $ has a variance $ \\sigma^2I $ (identity matrix scaled by $ \\sigma^2 $), the variance of $ \\hat{\\beta}^* $ is:\n",
    "$$ Var(\\hat{\\beta}^*) = Var(Hy) = Var(H(X\\beta + \\epsilon)) $$\n",
    "$$ Var(\\hat{\\beta}^*) = HVar(\\epsilon)H^T $$\n",
    "$$ Var(\\hat{\\beta}^*) = H(\\sigma^2I)H^T $$\n",
    "$$ Var(\\hat{\\beta}^*) = \\sigma^2HH^T $$\n",
    "$$ Var(\\hat{\\beta}^*) = \\sigma^2(X^TX)^{-1} $$ \n",
    "\n",
    "For $ \\hat{\\beta} $, we apply the same principle:\n",
    "$$ Var(\\hat{\\beta}) = Var(Cy) = Var(C(X\\beta + \\epsilon)) $$\n",
    "$$ Var(\\hat{\\beta}) = CVar(\\epsilon)C^T $$\n",
    "$$ Var(\\hat{\\beta}) = C(\\sigma^2I)C^T $$\n",
    "$$ Var(\\hat{\\beta}) = \\sigma^2CC^T $$\n",
    "$$ Var(\\hat{\\beta}) = \\sigma^2(H + D)(H + D)^T $$\n",
    "$$ Var(\\hat{\\beta}) = \\sigma^2(HH^T + HD^T + DH^T + DD^T) $$\n",
    "\n",
    "3. **Showing that $ Var(\\hat{\\beta}^*) \\leq Var(\\hat{\\beta}) $**\n",
    "\n",
    "From step 2, we have:\n",
    "$$ Var(\\hat{\\beta}^*) = \\sigma^2(X^TX)^{-1} $$\n",
    "$$ Var(\\hat{\\beta}) = \\sigma^2(HH^T + HD^T + DH^T + DD^T) $$\n",
    "\n",
    "Since $ H = (X^TX)^{-1}X^T $, $ HH^T = (X^TX)^{-1} $. Therefore, we can rewrite $ Var(\\hat{\\beta}) $ as:\n",
    "$$ Var(\\hat{\\beta}) = \\sigma^2((X^TX)^{-1} + HD^T + DH^T + DD^T) $$\n",
    "\n",
    "Comparing $ Var(\\hat{\\beta}^*) $ and $ Var(\\hat{\\beta}) $, we have:\n",
    "$$ Var(\\hat{\\beta}) - Var(\\hat{\\beta}^*) $$\n",
    "\n",
    "$$ Var(\\hat{\\beta}) - Var(\\hat{\\beta}^*) = \\sigma^2(HD^T + DH^T + DD^T) $$\n",
    "\n",
    "Since $ \\sigma^2 > 0 $ and $ DD^T $ is positive semi-definite (it's a square of a matrix), the difference $ Var(\\hat{\\beta}) - Var(\\hat{\\beta}^*) $ is positive semi-definite. This means that $ Var(\\hat{\\beta}^*) $ is less than or equal to $ Var(\\hat{\\beta}) $ because the term $ HD^T + DH^T + DD^T $ adds additional variance.\n",
    "\n",
    "To conclude that $ Var(\\hat{\\beta}^*) $ is indeed the smallest, we need to invoke the Gauss-Markov theorem. Under the Gauss-Markov assumptions (linear model, errors with zero mean, constant variance, and no autocorrelation), the OLS estimator $ \\hat{\\beta}^* $ is the best linear unbiased estimator (BLUE). That means, of all the linear unbiased estimators, $ \\hat{\\beta}^* $ has the smallest variance. This holds as long as the assumptions of the Gauss-Markov theorem are met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Biasing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Ridge regression adjusts the ordinary least squares (OLS) estimator to include a penalty on the size of the coefficients. This penalty shrinks the coefficients towards zero, which biases the estimates.\n",
    "\n",
    "The estimator $ \\beta_{\\text{ridge}}^* $ in ridge regression is given by:\n",
    "$$ \\beta_{\\text{ridge}}^* = \\text{argmin}_\\beta \\{ (y_c - X_c\\beta)^T(y_c - X_c\\beta) + \\lambda\\|\\beta\\|_2^2 \\} $$\n",
    "where $ y_c $ is the centered response variable, $ X_c $ is the matrix of centered predictors, and $ \\lambda $ is the regularization parameter.\n",
    "\n",
    "To find the closed-form solution of $ \\beta_{\\text{ridge}}^* $, we take the derivative of the above expression with respect to $ \\beta $, set it to zero, and solve for $ \\beta $:\n",
    "$$ 2X_c^T(X_c\\beta - y_c) + 2\\lambda\\beta = 0 $$\n",
    "$$ X_c^TX_c\\beta + \\lambda I\\beta = X_c^Ty_c $$\n",
    "$$ (X_c^TX_c + \\lambda I)\\beta = X_c^Ty_c $$\n",
    "$$ \\beta_{\\text{ridge}}^* = (X_c^TX_c + \\lambda I)^{-1}X_c^Ty_c $$\n",
    "\n",
    "Now, let's show that this estimator is biased. We compute the expected value of $ \\beta_{\\text{ridge}}^* $:\n",
    "$$ E[\\beta_{\\text{ridge}}^*] = E[(X_c^TX_c + \\lambda I)^{-1}X_c^Ty_c] $$\n",
    "\n",
    "Assuming the true model is $ y_c = X_c\\beta + \\epsilon $ where $ E[\\epsilon] = 0 $ and $ Var(\\epsilon) = \\sigma^2I $, we have:\n",
    "$$ E[\\beta_{\\text{ridge}}^*] = E[(X_c^TX_c + \\lambda I)^{-1}X_c^T(X_c\\beta + \\epsilon)] $$\n",
    "$$ E[\\beta_{\\text{ridge}}^*] = (X_c^TX_c + \\lambda I)^{-1}X_c^TX_c\\beta + (X_c^TX_c + \\lambda I)^{-1}X_c^TE[\\epsilon] $$\n",
    "$$ E[\\beta_{\\text{ridge}}^*] = (X_c^TX_c + \\lambda I)^{-1}(X_c^TX_c)\\beta $$\n",
    "\n",
    "Since $ (X_c^TX_c + \\lambda I)^{-1}(X_c^TX_c) $ is not the identity matrix, it follows that:\n",
    "$$ E[\\beta_{\\text{ridge}}^*] \\neq \\beta $$\n",
    "\n",
    "Therefore, $ \\beta_{\\text{ridge}}^* $ is a biased estimator of $ \\beta $. The bias arises from the shrinkage term $ \\lambda\\|\\beta\\|_2^2 $ which penalizes the size of the coefficients, hence the expected value of the ridge estimator does not equal the true parameter $ \\beta $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The SVD of the centered matrix $ X_c $ is $ X_c = UDV^T $, where $ U $ and $ V $ are orthogonal matrices containing the left-singular vectors and right-singular vectors, respectively, and $ D $ is a diagonal matrix containing the singular values.\n",
    "\n",
    "For ridge regression, we have the normal equation:\n",
    "\n",
    "$$ (X_c^TX_c + \\lambda I)\\beta = X_c^Ty_c $$\n",
    "\n",
    "Substitute $ X_c = UDV^T $ into the normal equation:\n",
    "\n",
    "$$ ((UDV^T)^T(UDV^T) + \\lambda I)\\beta = (UDV^T)^Ty_c $$\n",
    "$$ (VD^TUDV^T + \\lambda I)\\beta = VD^T U^Ty_c $$\n",
    "\n",
    "Since $ U $ and $ V $ are orthogonal matrices, $ U^TU = V^TV = I $. Therefore, we can simplify the equation to:\n",
    "\n",
    "$$ (VD^2V^T + \\lambda I)\\beta = VD^T U^Ty_c $$\n",
    "\n",
    "Multiply both sides by $ V^T $:\n",
    "\n",
    "$$ V^T(VD^2V^T + \\lambda I)\\beta = V^TVD^TU^Ty_c $$\n",
    "$$ (D^2 + \\lambda I)V^T\\beta = D^TU^Ty_c $$\n",
    "\n",
    "Now, $ D $ is a diagonal matrix, so $ D^2 $ is also diagonal, and we can represent $ D^2 + \\lambda I $ as another diagonal matrix $ \\Lambda $, where each entry $ \\Lambda_{ii} = D_{ii}^2 + \\lambda $.\n",
    "\n",
    "Let $ \\gamma = V^T\\beta $, then the equation becomes:\n",
    "\n",
    "$$ \\Lambda \\gamma = D^TU^Ty_c $$\n",
    "\n",
    "Since $ \\Lambda $ is diagonal, it's straightforward to solve for $ \\gamma $ by element-wise division on the diagonal:\n",
    "\n",
    "$$ \\gamma_i = \\frac{D_{ii}U_i^Ty_c}{D_{ii}^2 + \\lambda} $$\n",
    "\n",
    "Finally, we can solve for $ \\beta $ by multiplying by $ V $:\n",
    "\n",
    "$$ \\beta = V\\gamma $$\n",
    "\n",
    "This is the SVD solution for ridge regression.\n",
    "\n",
    "Using the SVD decomposition is particularly useful when $ X_c^TX_c $ is nearly singular or ill-conditioned, which means that some of the singular values in $ D $ are close to zero. The addition of $ \\lambda I $ in ridge regression (the ridge penalty) ensures that $ D^2 + \\lambda I $ does not have any zero entries, and hence $ \\Lambda $ is invertible. This avoids the instability in the inversion of $ X_c^TX_c $ that would otherwise occur due to small or zero singular values.\n",
    "\n",
    "Moreover, using SVD makes it computationally efficient to calculate the ridge regression solution, especially when dealing with large or complex datasets, since it avoids directly inverting a potentially large matrix $ X_c^TX_c $. The decomposition also provides numerical stability, which is critical in many practical machine learning applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "So we have:\n",
    "$$ Var(\\hat{\\beta}_{ridge}^*) = \\sigma^2 (X^TX + \\lambda I)^{-1} X^T X (X^TX + \\lambda I)^{-1} $$\n",
    "$$ Var(\\hat{\\beta}_{OLS}^*) = \\sigma^2 (X^TX)^{-1} $$\n",
    "\n",
    "*Showing that $ (X^TX + \\lambda I)^{-1} $ is smaller than $ (X^TX)^{-1} $*\n",
    "\n",
    "The matrix $ (X^TX + \\lambda I) $ is more \"spread out\" than $ X^TX $ since $ \\lambda I $ adds a positive constant to the diagonal elements of $ X^TX $, increasing its eigenvalues. When you invert this matrix, the eigenvalues of the inverse are smaller than those of $ (X^TX)^{-1} $ because the inversion of a matrix is equivalent to inverting its eigenvalues.\n",
    "\n",
    "So we have the relationship:\n",
    "$$ (X^TX + \\lambda I)^{-1} \\leq (X^TX)^{-1} $$\n",
    "\n",
    "This is due to the fact that for any positive definite matrix $ A $ and positive scalar $ \\lambda $, $ A^{-1} $ will have larger eigenvalues than $ (A + \\lambda I)^{-1} $.\n",
    "\n",
    "**Comparing the variances**\n",
    "\n",
    "To compare the variances of the OLS and ridge estimators, note that the additional term $ \\lambda I $ in the ridge estimator causes the variance to be scaled down by the matrix $ (X^TX + \\lambda I)^{-1} $. \n",
    "\n",
    "So we have:\n",
    "$$ Var(\\hat{\\beta}_{ridge}^*) = \\sigma^2 (X^TX + \\lambda I)^{-1} X^T X (X^TX + \\lambda I)^{-1} $$\n",
    "$$ Var(\\hat{\\beta}_{OLS}^*) = \\sigma^2 (X^TX)^{-1} $$\n",
    "\n",
    "Because $ (X^TX + \\lambda I)^{-1} \\leq (X^TX)^{-1} $, it follows that:\n",
    "$$ \\sigma^2 (X^TX + \\lambda I)^{-1} X^T X (X^TX + \\lambda I)^{-1} \\leq \\sigma^2 (X^TX)^{-1} $$\n",
    "\n",
    "Therefore:\n",
    "$$ Var(\\hat{\\beta}_{ridge}^*) \\leq Var(\\hat{\\beta}_{OLS}^*) $$\n",
    "\n",
    "This proves that the variance of the ridge regression estimator is less than or equal to the variance of the OLS estimator. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias and Variance trade-off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For a new test point $ (x_0, y_0) $, the mean squared error of the prediction using ridge regression is:\n",
    "\n",
    "$$ MSE = E[(y_0 - x_0^T \\hat{\\beta}_{ridge}^*)^2] $$\n",
    "\n",
    "Given that $ y_0 = x_0^T\\beta + \\epsilon_0 $, where $ \\epsilon_0 $ is the error term with mean 0 and variance $ \\sigma^2 $, we can expand the MSE as follows:\n",
    "\n",
    "$$ MSE = E[((x_0^T\\beta + \\epsilon_0) - x_0^T \\hat{\\beta}_{ridge}^*)^2] $$\n",
    "$$ MSE = E[(\\epsilon_0 - x_0^T (\\hat{\\beta}_{ridge}^* - \\beta))^2] $$\n",
    "$$ MSE = E[\\epsilon_0^2] + E[(x_0^T (\\hat{\\beta}_{ridge}^* - \\beta))^2] - 2E[\\epsilon_0 x_0^T (\\hat{\\beta}_{ridge}^* - \\beta)] $$\n",
    "Since $ \\epsilon_0 $ and $ \\hat{\\beta}_{ridge}^* $ are independent, and $ E[\\epsilon_0] = 0 $, the cross-term disappears and we get:\n",
    "$$ MSE = \\sigma^2 + E[(x_0^T (\\hat{\\beta}_{ridge}^* - \\beta))^2] $$\n",
    "\n",
    "The first term is the variance of the error term $ \\epsilon_0 $, and the second term is the squared bias of the ridge estimator.\n",
    "\n",
    "**Bias**:\n",
    "Bias of the estimator is the difference between the expected prediction and the true value:\n",
    "\n",
    "$$ Bias = E[x_0^T \\hat{\\beta}_{ridge}^*] - x_0^T\\beta $$\n",
    "Since $ \\hat{\\beta}_{ridge}^* $ is biased, as $ \\lambda $ increases, the shrinkage effect on the coefficients becomes stronger, which means that $ \\hat{\\beta}_{ridge}^* $ is pulled further away from $ \\beta $, increasing the bias.\n",
    "\n",
    "**Variance**:\n",
    "Variance of the estimator is the variability of the prediction around its expected value:\n",
    "\n",
    "$$ Variance = E[(x_0^T \\hat{\\beta}_{ridge}^* - E[x_0^T \\hat{\\beta}_{ridge}^*])^2] $$\n",
    "As $ \\lambda $ increases, the estimates of $ \\hat{\\beta}_{ridge}^* $ become more stable and less sensitive to the particular sample because the shrinkage effect reduces the impact of any one data point or feature, decreasing the variance of the estimator.\n",
    "\n",
    "**Impact of Increasing $ \\lambda $ on MSE**:\n",
    "\n",
    "- **Bias**: Increases with $ \\lambda $ because the shrinkage effect leads to more substantial underestimation or overestimation of the true coefficients.\n",
    "- **Variance**: Decreases with $ \\lambda $ because the estimator becomes more stable and less sensitive to fluctuations in the data.\n",
    "- **MSE**: The MSE is the sum of the variance of the estimator and the square of its bias, plus the irreducible error $ \\sigma^2 $. As $ \\lambda $ increases, there's a trade-off between increasing bias and decreasing variance. Initially, as $ \\lambda $ increases from 0, the decrease in variance might lead to a decrease in MSE until a certain point where the increase in bias becomes dominant, and the MSE starts to increase.\n",
    "\n",
    "Therefore, the overall impact of $ \\lambda $ on MSE will depend on the relative rates at which the bias is increasing and the variance is decreasing. There is typically an optimal value of $ \\lambda $ that minimizes the MSE, balancing the trade-off between bias and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relation between estimators\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Substituing the condition into the OLS estimator**:\n",
    "If $ X_c^TX_c = I_d $, then the OLS estimator simplifies to:\n",
    "$$ \\hat{\\beta}^*_{OLS} = I_d^{-1}X_c^Ty $$\n",
    "$$ \\hat{\\beta}^*_{OLS} = X_c^Ty $$\n",
    "\n",
    "**Substituing the condition into the ridge regression estimator**:\n",
    "Similarly, substituting $ X_c^TX_c = I_d $ into the ridge regression estimator gives:\n",
    "$$ \\hat{\\beta}^*_{ridge} = (I_d + \\lambda I_d)^{-1}X_c^Ty $$\n",
    "$$ \\hat{\\beta}^*_{ridge} = (I_d + \\lambda I_d)^{-1}\\hat{\\beta}^*_{OLS} $$\n",
    "Here, $ I_d $ is the identity matrix of size $ d \\times d $, and $ \\lambda I_d $ is a diagonal matrix where each diagonal element is $ \\lambda $.\n",
    "\n",
    "**Finding the inverse of $ I_d + \\lambda I_d $**:\n",
    "The matrix $ I_d + \\lambda I_d $ is also a diagonal matrix, where each diagonal entry is $ 1+\\lambda $. The inverse of a diagonal matrix is simply the diagonal matrix with each diagonal element replaced by its reciprocal:\n",
    "$$ (I_d + \\lambda I_d)^{-1} = \\frac{1}{1+\\lambda}I_d $$\n",
    "\n",
    "**Final expression for $ \\hat{\\beta}^*_{ridge} $**:\n",
    "Substituting back into the equation for $ \\hat{\\beta}^*_{ridge} $, we get:\n",
    "$$ \\hat{\\beta}^*_{ridge} = \\frac{1}{1+\\lambda}I_d \\hat{\\beta}^*_{OLS} $$\n",
    "$$ \\hat{\\beta}^*_{ridge} = \\frac{\\hat{\\beta}^*_{OLS}}{1+\\lambda} $$\n",
    "\n",
    "This shows that under the condition that $ X_c^TX_c = I_d $, the ridge regression estimator is a scaled version of the OLS estimator, with the scaling factor being $ \\frac{1}{1+\\lambda} $. The scaling factor shrinks the OLS estimator towards zero, which is consistent with the bias introduced by the ridge penalty $ \\lambda $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **Starting with the OLS solution under the given condition**:\n",
    "If $ X_c^T X_c = I_d $, the OLS solution $ \\hat{\\beta}_{OLS}^* $ is:\n",
    "$$ \\hat{\\beta}_{OLS}^* = (X_c^T X_c)^{-1}X_c^Ty $$\n",
    "$$ \\hat{\\beta}_{OLS}^* = I_d^{-1}X_c^Ty $$\n",
    "$$ \\hat{\\beta}_{OLS}^* = X_c^Ty $$\n",
    "\n",
    "2. **Writing down the objective function with the condition applied**:\n",
    "When $ X_c^T X_c = I_d $, the Elastic Net objective function becomes:\n",
    "\n",
    "$$ \\hat{\\beta}_{ENet}^* = \\text{argmin}_\\beta \\left\\{ (y_c - \\beta)^T(y_c - \\beta) + \\lambda_2\\|\\beta\\|_2^2 + \\lambda_1\\|\\beta\\|_1 \\right\\} $$\n",
    "\n",
    "3. **Differentiating with respect to $ \\beta $**:\n",
    "To find the minimizer of this function, we would typically take the derivative with respect to $ \\beta $. However, due to the presence of the L1 term (which is not differentiable), we cannot use this method directly. Instead, we find the solution by considering the the subgradient of the objective function, which accounts for both the L2 and L1 terms.\n",
    "\n",
    "4. **Considering the subgradient for the L1 norm:**\n",
    "\n",
    "The L1 norm is not differentiable at 0, but we can use the concept of subgradient for optimization. A subgradient at a point for the L1 term $ \\lambda_1 \\|\\beta\\|_1 $ is $ \\lambda_1 \\text{sign}(\\beta_j) $ for each $ \\beta_j \\neq 0 $ and is within the interval $ [-\\lambda_1, \\lambda_1] $ for each $ \\beta_j = 0 $.\n",
    "\n",
    "5. **Setting up the optimality condition for minimization:**\n",
    "\n",
    "The optimality condition, including the subgradient for the non-differentiable part, is given by:\n",
    "\n",
    "$$ 0 \\in 2(y_c - \\beta) + 2\\lambda_2 \\beta + \\lambda_1 \\text{sign}(\\beta) $$\n",
    "\n",
    "6. **Isolating terms involving $ \\beta $:**\n",
    "\n",
    "$$ 2\\lambda_2 \\beta + \\lambda_1 \\text{sign}(\\beta) = 2y_c $$\n",
    "\n",
    "7. **Solving for $ \\beta $:**\n",
    "\n",
    "$$ \\beta = \\frac{2y_c - \\lambda_1 \\text{sign}(\\beta)}{2\\lambda_2} $$\n",
    "\n",
    "Since $ y_c = X_c^T y $ and $ X_c^T X_c = I_d $, we substitute $ y_c $ with $ \\hat{\\beta}_{OLS}^* $:\n",
    "\n",
    "$$ \\beta = \\frac{\\hat{\\beta}_{OLS}^* - \\frac{\\lambda_1}{2} \\text{sign}(\\beta)}{1 + \\lambda_2} $$\n",
    "\n",
    "8. **Interpreting the solution:**\n",
    "\n",
    "This solution suggests that the Elastic Net estimator is a shrinkage of the OLS estimator towards zero by $ \\frac{\\lambda_1}{2(1+\\lambda_2)} $ in the direction given by the sign of the OLS estimator, with further scaling by $ \\frac{\n",
    "\n",
    "1}{1+\\lambda_2} $. It's a soft-thresholding operation that can set coefficients to zero if they are small enough, combining the L1 norm's sparsity with the L2 norm's shrinkage.\n",
    "\n",
    "However, the complete solution to the Elastic Net problem typically requires numerical methods, especially when $ X_c $ is not an identity matrix, because the L1 norm makes the problem non-differentiable where $ \\beta = 0 $. The soft-thresholding is derived from the subdifferential of the L1 norm term at $ \\beta_j $:\n",
    "\n",
    "$$ \\partial(\\lambda_1 |\\beta_j|) = \n",
    "\\begin{cases} \n",
    "\\{-\\lambda_1\\} & \\text{if } \\beta_j < 0,\\\\\n",
    "[-\\lambda_1, \\lambda_1] & \\text{if } \\beta_j = 0,\\\\\n",
    "\\{\\lambda_1\\} & \\text{if } \\beta_j > 0. \n",
    "\\end{cases}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
